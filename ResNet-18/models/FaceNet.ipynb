{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from torchvision.models import ResNet18_Weights\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import BytesIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the FaceNet model with pre-trained ResNet18\n",
    "class FaceNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FaceNet, self).__init__()\n",
    "        # Load the pre-trained ResNet18 model with updated 'weights' argument\n",
    "        self.resnet = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        # Remove the last fully connected layer (fc layer)\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])\n",
    "        # Get the number of input features for the new embedding layer from the original resnet.fc layer\n",
    "        num_ftrs = models.resnet18(weights=ResNet18_Weights.DEFAULT).fc.in_features\n",
    "        # Add a new fully connected layer for embeddings\n",
    "        self.embedding_layer = nn.Linear(num_ftrs, 128)  # Output size of 128\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)  # Get features from ResNet\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = self.embedding_layer(x)  # Get embeddings\n",
    "        return x\n",
    "    \n",
    "    def get_face_embeddings(self, image_tensor):\n",
    "        self.eval()  # Set model to evaluation mode\n",
    "        with torch.no_grad():  # Disable gradient calculation for inference\n",
    "            embeddings = self(image_tensor)\n",
    "        return embeddings\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to calculate cosine similarity between two embeddings\n",
    "def cosine_similarity(embedding1, embedding2):\n",
    "    return F.cosine_similarity(embedding1, embedding2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing function to prepare images for the model\n",
    "def preprocess_image_from_url(url):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resizing to ResNet's expected size\n",
    "        transforms.ToTensor(),  # Convert image to Tensor\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalization for ResNet\n",
    "    ])\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers, verify=False)\n",
    "    \n",
    "    # Check if the response is successful\n",
    "    if response.status_code != 200:\n",
    "        raise ValueError(f\"Failed to fetch image from {url} (status code: {response.status_code})\")\n",
    "    \n",
    "    # Check the content type\n",
    "    content_type = response.headers.get('Content-Type')\n",
    "    if 'image' not in content_type:\n",
    "        raise ValueError(f\"Expected image, but got {content_type} from {url}\")\n",
    "    \n",
    "    # Open and preprocess the image\n",
    "    img = Image.open(BytesIO(response.content)).convert('RGB')  # Open and convert to RGB\n",
    "    img_tensor = preprocess(img).unsqueeze(0)  # Add batch dimension\n",
    "    return img_tensor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image URLs\n",
    "url1 = \"https://upload.wikimedia.org/wikipedia/commons/9/90/Brad_Pitt-69858.jpg\"  # Brad Pitt\n",
    "url2 = \"https://upload.wikimedia.org/wikipedia/commons/7/71/George_Clooney-69990.jpg\"  # George Clooney\n",
    "# Get embeddings for both images\n",
    "image_tensor1 = preprocess_image_from_url(url1)\n",
    "image_tensor2 = preprocess_image_from_url(url2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Load the FaceNet model\n",
    "facenet_model = FaceNet()\n",
    "facenet_model.eval()\n",
    "embeddings1 = facenet_model.get_face_embeddings( image_tensor1)\n",
    "embeddings2 = facenet_model.get_face_embeddings( image_tensor2)\n",
    "# Compute the cosine similarity between the embeddings\n",
    "similarity = cosine_similarity(embeddings1, embeddings2)\n",
    "print(f\"Cosine Similarity between the faces: {similarity.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(facenet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchinfo import summary  # For detailed model summary\n",
    "from fvcore.nn import FlopCountAnalysis  # For FLOPS calculation\n",
    "\n",
    "# Print model summary\n",
    "summary(facenet_model, input_size=(1, 3, 224, 224), col_names=[\"input_size\", \"output_size\", \"num_params\"], verbose=2)\n",
    "\n",
    "# Calculate FLOPs using fvcore\n",
    "input_tensor = torch.randn(1, 3, 224, 224)  # Example input tensor\n",
    "flop_counter = FlopCountAnalysis(facenet_model, input_tensor)\n",
    "\n",
    "# Show only inference FLOPs\n",
    "total_flops = flop_counter.total()  # Get total FLOPs for inference\n",
    "print(f\"Total GFLOPs: {total_flops/ 1e9} GFLOPs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from torch.nn import functional as F\n",
    "from torchprofile import profile_macs  # Assumes torchprofile is installed\n",
    "from torchvision.models import ResNet18_Weights\n",
    "\n",
    "\n",
    "# Step 2: Calculate FLOPs\n",
    "# Example input tensor of the shape that ResNet18 expects (batch size = 1, 3 color channels, 224x224 image)\n",
    "input_tensor = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# Profile the model to calculate MACs/FLOPs\n",
    "flops = profile_macs(facenet_model, input_tensor) * 2  # Convert MACs to FLOPs\n",
    "\n",
    "# Convert FLOPs to GFLOPs\n",
    "gflops = flops / 1e9\n",
    "print(f\"Total GFLOPs for inference: {gflops:.2f} GFLOPs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchinfo import summary  # For detailed model summary\n",
    "import torchprofile as tp      # Import the module correctly\n",
    "\n",
    "# Print model summary using torchinfo\n",
    "summary(facenet_model, input_size=(1, 3, 224, 224))\n",
    "\n",
    "# Use torchprofile to calculate FLOPs and parameters\n",
    "#flops_ = tp.profile(facenet_model, input_size=(1, 3, 224, 224))  # Use the correct reference to the profile function\n",
    "#print(f\"Total FLOPs: {flops_}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.onnx\n",
    "\n",
    "def export_model_to_onnx(model, input_size=(1, 3, 224, 224), output_path=\"facenet_resnet18.onnx\"):\n",
    "    \"\"\"\n",
    "    Export the PyTorch model to ONNX format without dynamic batching.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The PyTorch model to export\n",
    "    - input_size: The size of the input tensor (default is for ResNet18 input)\n",
    "    - output_path: Path where the ONNX model will be saved\n",
    "    \"\"\"\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Create a dummy input tensor with the specified size\n",
    "    dummy_input = torch.randn(input_size)\n",
    "    \n",
    "    # Export the model to ONNX\n",
    "    torch.onnx.export(\n",
    "        model,                               # Model to export\n",
    "        dummy_input,                         # Dummy input tensor\n",
    "        output_path,                         # Output file path\n",
    "        export_params=True,                  # Store the trained parameter weights\n",
    "        opset_version=11,                    # ONNX version to export to\n",
    "        do_constant_folding=True,            # Whether to apply constant folding optimization\n",
    "        input_names=['input'],               # Model's input name\n",
    "        output_names=['output']              # Model's output name\n",
    "    )\n",
    "    \n",
    "    print(f\"Model exported to ONNX format and saved as {output_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "# Define model path and ensure directory exists\n",
    "model_path = \"onnx/facenet_resnet18.onnx\"\n",
    "model_dir = os.path.dirname(model_path)\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "def export_model_to_onnx(model, input_size=(1, 3, 224, 224), output_path=model_path):\n",
    "    dummy_input = torch.randn(*input_size)  # Dummy input for model tracing\n",
    "    torch.onnx.export(model, dummy_input, output_path, opset_version=11)\n",
    "    print(f\"Model exported to {output_path}\")\n",
    "\n",
    "# Assuming facenet_model is defined and loaded with weights\n",
    "export_model_to_onnx(facenet_model, input_size=(1, 3, 224, 224), output_path=model_path)\n",
    "\n",
    "# Load and check the exported ONNX model\n",
    "onnx_model = onnx.load(pathlib.Path(model_path))\n",
    "onnx.checker.check_model(onnx_model)\n",
    "print(\"ONNX model is valid.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import netron\n",
    "def show_model(model_file_name,itf='localhost',port=8098):\n",
    "    netron.start(file=model_file_name,address=(itf,port))\n",
    "    return port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "port=show_model(\"./\"+str(model_path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isolde-cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
